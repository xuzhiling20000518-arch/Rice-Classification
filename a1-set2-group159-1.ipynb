{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Rice Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group number: A1 group-set2 159\n",
    "##### Student 1 SID: 540825875\n",
    "##### Student 2 SID: 530683980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split,GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12573' '461.4660034' '192.9033508' '84.57207489' '0.898771763' '12893'\n",
      "  '0.550433397' 'class2']\n",
      " ['12845' '464.1210022' '194.3322144' '85.52433777' '0.897951961' '13125'\n",
      "  '0.774962306' 'class2']\n",
      " ['14055' '488.7489929' '207.7517548' '87.25032806' '0.907536149' '14484'\n",
      "  '0.550076306' 'class1']\n",
      " ['14412' '490.3240051' '207.4761353' '89.68951416' '0.901735425' '14703'\n",
      "  '0.598853171' 'class1']\n",
      " ['14658' '477.1170044' '189.5666351' '99.99777985' '0.849550545' '15048'\n",
      "  '0.649503708' 'class2']]\n"
     ]
    }
   ],
   "source": [
    "# Load the rice dataset: rice-final2.csv\n",
    "rice_dataset = np.genfromtxt('rice-final2.csv', delimiter=',', skip_header=1, dtype=None, encoding=None)\n",
    "print(rice_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset\n",
    "\n",
    "# encode the class value\n",
    "rice_dataset = np.where(rice_dataset == 'class1', '0', rice_dataset)\n",
    "rice_dataset = np.where(rice_dataset == 'class2', '1', rice_dataset)\n",
    "rice_dataset = np.where(rice_dataset == '?', np.nan, rice_dataset)\n",
    "\n",
    "#replace missing values with means\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(rice_dataset)\n",
    "rice_dataset = imp_mean.transform(rice_dataset)\n",
    "\n",
    "#normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(rice_dataset)\n",
    "rice_dataset = scaler.transform(rice_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4628,0.5406,0.5113,0.4803,0.7380,0.4699,0.1196,1.0\n",
      "0.4900,0.5547,0.5266,0.5018,0.7319,0.4926,0.8030,1.0\n",
      "0.6109,0.6847,0.6707,0.5409,0.8032,0.6253,0.1185,0.0\n",
      "0.6466,0.6930,0.6677,0.5961,0.7601,0.6467,0.2669,0.0\n",
      "0.6712,0.6233,0.4755,0.8293,0.3721,0.6803,0.4211,1.0\n",
      "0.2634,0.2932,0.2414,0.4127,0.5521,0.2752,0.2825,1.0\n",
      "0.8175,0.9501,0.9515,0.5925,0.9245,0.8162,0.0000,0.0\n",
      "0.3174,0.3588,0.3601,0.3908,0.6921,0.3261,0.8510,1.0\n",
      "0.3130,0.3050,0.2150,0.5189,0.3974,0.3159,0.4570,1.0\n",
      "0.5120,0.5237,0.4409,0.6235,0.5460,0.5111,0.3155,1.0\n"
     ]
    }
   ],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "\n",
    "#slice the dataset in X and y\n",
    "X = rice_dataset[:, :-1]\n",
    "y = rice_dataset[:, -1]\n",
    "\n",
    "print_data(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    scores = []\n",
    "\n",
    "    #split data into training and test sets by cross-validation\n",
    "    for train_index, test_index in cvKFold.split(X, y):\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        #make predictions after training\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        #score the accuracy of the model\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "    #output the mean of scores\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "    clf = GaussianNB()\n",
    "    scores = []\n",
    "    for train_index, test_index in cvKFold.split(X, y):\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "    scores = []\n",
    "    for train_index, test_index in cvKFold.split(X, y):\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    clf = BaggingClassifier(estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=0), n_estimators=n_estimators, max_samples=max_samples, random_state=0)\n",
    "    scores = []\n",
    "    for train_index, test_index in cvKFold.split(X, y):\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=0), n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = []\n",
    "    for train_index, test_index in cvKFold.split(X, y):\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = []\n",
    "    for train_index, test_index in cvKFold.split(X, y):\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR average cross-validation accuracy:  0.9386\n",
      "NB average cross-validation accuracy:  0.9264\n",
      "DT average cross-validation accuracy:  0.9179\n",
      "Bagging average cross-validation accuracy:  0.9414\n",
      "AdaBoost average cross-validation accuracy:  0.9250\n",
      "GB average cross-validation accuracy:  0.9300\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 5\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: \", f\"{logregClassifier(X, y):.4f}\")\n",
    "print(\"NB average cross-validation accuracy: \", f\"{nbClassifier(X, y):.4f}\")\n",
    "print(\"DT average cross-validation accuracy: \", f\"{dtClassifier(X, y):.4f}\")\n",
    "print(\"Bagging average cross-validation accuracy: \", f\"{bagDTClassifier(X, y, bag_n_estimators, bag_max_samples, bag_max_depth):.4f}\")\n",
    "print(\"AdaBoost average cross-validation accuracy: \", f\"{adaDTClassifier(X, y, ada_n_estimators, ada_learning_rate, ada_bag_max_depth):.4f}\")\n",
    "print(\"GB average cross-validation accuracy: \", f\"{gbClassifier(X, y, gb_n_estimators, gb_learning_rate):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7]\n",
    "p = [1, 2]\n",
    "\n",
    "def bestKNNClassifier(X, y):    \n",
    "    # Dataset split,set random_state ensures repeatability\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)    \n",
    "    #10 fold stratified cross-validation\n",
    "    cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    # Create a grid search model，using knn classifier\n",
    "    knn = KNeighborsClassifier()\n",
    "    param_grid = {'n_neighbors': k, 'p': p}\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    \n",
    "    # All parameter combinations are traversed on the training set to extract the optimal parameter group\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_k = grid_search.best_params_['n_neighbors']\n",
    "    best_p = grid_search.best_params_['p']\n",
    "    # Output the required model evaluation metrics\n",
    "    cv_acc = grid_search.best_score_\n",
    "    test_acc = grid_search.score(X_test, y_test)\n",
    "    \n",
    "    return best_k, best_p, cv_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "C = [0.01, 0.1, 1, 5] \n",
    "gamma = [0.01, 0.1, 1, 10]\n",
    "\n",
    "def bestSVMClassifier(X, y):\n",
    "    # Dataset split,set random_state ensures repeatability\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)    \n",
    "    cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    # Create a grid search model，using svm classifier\n",
    "    svm = SVC(kernel='rbf', random_state=0)\n",
    "    param_grid = {'C': C, 'gamma': gamma}\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=cvKFold, scoring='accuracy')   \n",
    "    # All parameter combinations are traversed on the training set to extract the optimal parameter group\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_C = grid_search.best_params_['C']\n",
    "    best_gamma = grid_search.best_params_['gamma']  \n",
    "    #Output the required model evaluation metrics\n",
    "    cv_acc = grid_search.best_score_\n",
    "    test_acc = grid_search.score(X_test, y_test)\n",
    "    \n",
    "    return best_C, best_gamma, cv_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 30, 60, 100]\n",
    "max_leaf_nodes = [6, 12]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    # Dataset split,set random_state ensures repeatability\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    # Create a grid search model，using rf classifier\n",
    "    RF = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=0)\n",
    "    param_grid = {'n_estimators': n_estimators, 'max_leaf_nodes': max_leaf_nodes}\n",
    "    grid_search = GridSearchCV(RF, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_n = grid_search.best_params_['n_estimators']\n",
    "    best_leaf = grid_search.best_params_['max_leaf_nodes']\n",
    "    #Output the required model evaluation metrics, have macro and weighted F1 score\n",
    "    cv_acc = grid_search.best_score_\n",
    "    test_acc = grid_search.score(X_test, y_test)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return best_n, best_leaf, cv_acc, test_acc, macro_f1, weighted_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 5\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.9371\n",
      "KNN test set accuracy: 0.9257\n",
      "\n",
      "SVM best C: 5.0000\n",
      "SVM best gamma: 1.0000\n",
      "SVM cross-validation accuracy: 0.9457\n",
      "SVM test set accuracy: 0.9343\n",
      "\n",
      "RF best n_estimators: 30\n",
      "RF best max_leaf_nodes: 12\n",
      "RF cross-validation accuracy: 0.9390\n",
      "RF test set accuracy: 0.9371\n",
      "RF test set macro average F1: 0.9355\n",
      "RF test set weighted average F1: 0.9370\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "\n",
    "\n",
    "# call the three classification model functions\n",
    "best_k, best_p, knn_cv, knn_test = bestKNNClassifier(X, y)\n",
    "best_C, best_gamma, svm_cv, svm_test = bestSVMClassifier(X, y)\n",
    "best_n_est, best_max_leaf, rf_cv, rf_test, rf_f1_macro, rf_f1_weighted = bestRFClassifier(X, y)\n",
    "print(\"KNN best k: {}\".format(best_k))\n",
    "print(\"KNN best p: {}\".format(best_p))\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(knn_cv))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(knn_test))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"SVM best C: {:.4f}\".format(best_C))\n",
    "print(\"SVM best gamma: {:.4f}\".format(best_gamma))\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\".format(svm_cv))\n",
    "print(\"SVM test set accuracy: {:.4f}\".format(svm_test))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"RF best n_estimators: {}\".format(best_n_est)) \n",
    "print(\"RF best max_leaf_nodes: {}\".format(best_max_leaf))\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(rf_cv))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(rf_test))\n",
    "print(\"RF test set macro average F1: {:.4f}\".format(rf_f1_macro))\n",
    "print(\"RF test set weighted average F1: {:.4f}\".format(rf_f1_weighted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write one paragraph describing the most important thing that you have learned throughout this assignment.\n",
    "\n",
    "##### Student 1: In this assignment, I’ve learned that when using classification algorithms represented by classifiers, the most complicated computational details have already been covered. Instead, the essential task for users is to preprocess the data (including dealing with missing values, normalization and encoding the class), as well as choose and adjust algorithm parameters within the core of the algorithms. The routine to follow is: import the classifier, split the data into training and test sets, make predictions after training the classifier, and score the accuracy of the model at last. On the other hand, using def to encapsulate this routine is very useful, as it provides a clear structure that is easy to use. Additionally, replacing the class values first can avoid affecting the normalization process and the handling of missing values. It’s also better to use slicing without specifying specific columns and rows to accommodate different datasets.\n",
    "\n",
    "##### Student 2: In this assignment, the most significant aspect I have learned is the assistance of hyperparameter tuning in enhancing model performance. I have mastered the utilization of GridSearchCV and StratifiedKFold methods. Stratified cross-validation can assist in addressing the issue of class imbalance, while the automated search of GridSearchCV can facilitate the automatic comparison of multiple parameter combinations for the selection of the optimal solution. Subsequently, through my comparison of the outcomes of the three models, the Random Forest model yielded the best result. I think the reason may be that all the attributes of the dataset are numerical, and random forest based on ensemble learning of decision trees is less sensitive to the size of the data than the other two methods, so it performs best. Additionally, during the process of completing the code, I felt that encapsulating each model in the same function can help improve the logic and tidiness of the code. I should develop the good habit of encapsulating functions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
